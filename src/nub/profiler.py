"""
File profiler - detect state features for optimal policy selection.

Maps file characteristics to state space dimensions from NUB_USAGE_GUIDE.md
"""

from __future__ import annotations

import re
from pathlib import Path


def profile_file(filepath: str | Path) -> dict[str, str | int | float | list]:
    """
    Profile a file to detect state features for exploration policy selection.

    Returns dict with state features:
    - scale: < 20K | 20K-200K | 200K-2M | > 2M tokens
    - structure: high | medium | low
    - density: sparse | moderate | dense
    - line_length: short | mixed | long
    - clustering: topical | chronological | random
    - separators: detected separator patterns

    Plus numeric stats for reference.
    """
    path = Path(filepath)

    if not path.exists():
        return {"error": "File not found"}

    try:
        content = path.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        return {"error": str(e)}

    lines = content.split("\n")
    file_size = path.stat().st_size
    char_count = len(content)

    # Estimate tokens (rough: 1 token ≈ 4 chars)
    est_tokens = char_count // 4

    # Basic stats
    num_lines = len(lines)
    non_empty_lines = [line for line in lines if line.strip()]
    avg_line_len = sum(len(line) for line in lines) / num_lines if num_lines > 0 else 0

    # State features
    scale = _detect_scale(est_tokens)
    structure = _detect_structure(lines, content)
    density = _detect_density(content, file_size, avg_line_len)
    line_length = _detect_line_length(lines, avg_line_len)
    clustering = _detect_clustering(lines, content)
    separators = _detect_separators(lines, content)

    return {
        # State features (categorical)
        "scale": scale,
        "structure": structure,
        "density": density,
        "line_length": line_length,
        "clustering": clustering,

        # Separator detection
        "separators": separators,

        # Numeric stats (for reference)
        "file_size_bytes": file_size,
        "num_lines": num_lines,
        "est_tokens": est_tokens,
        "avg_line_len": round(avg_line_len, 1),
        "blank_ratio": round(1 - len(non_empty_lines) / num_lines, 3) if num_lines > 0 else 0,
    }


def _detect_scale(tokens: int) -> str:
    """Detect file scale regime."""
    if tokens < 20_000:
        return "< 20K"
    elif tokens < 200_000:
        return "20K-200K"
    elif tokens < 2_000_000:
        return "200K-2M"
    else:
        return "> 2M"


def _detect_structure(lines: list[str], content: str) -> str:
    """Detect structural regularity (how exploitable patterns are)."""
    structure_score = 0

    # High structure indicators
    # 1. Numbered lists
    numbered_pattern = r'^\s*(\[?\d+[\.\):\]]|^\d+\.|\[\d+\])'
    numbered_lines = sum(1 for line in lines[:100] if re.match(numbered_pattern, line))
    if numbered_lines > 10:
        structure_score += 2
    elif numbered_lines > 3:
        structure_score += 1

    # 2. Headers (markdown, etc)
    header_pattern = r'^(#{1,6}\s+|={3,}$|-{3,}$|[A-Z][^a-z]{10,}$)'
    header_lines = sum(1 for line in lines if re.match(header_pattern, line))
    if header_lines > 10:
        structure_score += 2
    elif header_lines > 3:
        structure_score += 1

    # 3. CSV/TSV patterns
    if any(line.count(',') > 3 or line.count('\t') > 2 for line in lines[:20]):
        structure_score += 2

    # 4. Code structure
    code_markers = ['def ', 'class ', 'function ', 'interface ', 'struct ', 'impl ']
    if sum(1 for line in lines[:100] if any(marker in line for marker in code_markers)) > 5:
        structure_score += 1

    # 5. Consistent indentation
    indent_lines = [line for line in lines if line and line[0] in (' ', '\t')]
    if len(indent_lines) > len(lines) * 0.3:
        structure_score += 1

    # Map to categories
    if structure_score >= 3:
        return "high"
    elif structure_score >= 1:
        return "medium"
    else:
        return "low"


def _detect_density(content: str, file_size: int, avg_line_len: float) -> str:
    """Detect meaning density per token."""
    density_score = 0

    # Sparse indicators (URLs, boilerplate, repetition)
    url_count = len(re.findall(r'https?://\S+', content))
    if url_count > 50:
        return "sparse"  # URL-heavy file

    # Check for boilerplate patterns
    sample = content[:5000].lower()
    boilerplate_markers = [
        'copyright', 'license', 'all rights reserved',
        'generated by', 'auto-generated', 'do not edit',
    ]
    if sum(1 for marker in boilerplate_markers if marker in sample) >= 2:
        density_score -= 1

    # Dense indicators (technical terms, symbols, punctuation)
    technical_pattern = r'[A-Z]{2,}|[α-ω]|\$\w+|\b[a-z]+_[a-z]+\b'
    if len(re.findall(technical_pattern, content[:2000])) > 50:
        density_score += 1

    # Mathematical notation
    if content.count('∈') + content.count('∀') + content.count('∃') > 5:
        density_score += 2

    # Code density (lots of symbols)
    symbol_density = (content.count('{') + content.count(';') + content.count('[')) / len(content)
    if symbol_density > 0.02:
        density_score += 1

    # Average line length can indicate density
    if avg_line_len > 200:
        density_score -= 1  # Long lines often indicate sparse content (URLs, etc)
    elif avg_line_len > 80 and avg_line_len < 150:
        density_score += 1  # Good density

    if density_score >= 2:
        return "dense"
    elif density_score <= -1:
        return "sparse"
    else:
        return "moderate"


def _detect_line_length(lines: list[str], avg_line_len: float) -> str:
    """Detect line length distribution."""
    if not lines:
        return "short"

    # Calculate variance
    line_lengths = [len(line) for line in lines]
    max_len = max(line_lengths) if line_lengths else 0

    # Check for extremely long lines
    long_lines = sum(1 for length in line_lengths if length > 200)

    if long_lines > len(lines) * 0.3:  # > 30% of lines are long
        return "long"
    elif max_len > 500 or (long_lines > 0 and avg_line_len > 100):
        return "mixed"
    else:
        return "short"


def _detect_clustering(lines: list[str], content: str) -> str:
    """Detect content locality/organization."""
    # Chronological indicators
    timestamp_patterns = [
        r'\d{4}-\d{2}-\d{2}',  # ISO dates
        r'\d{10,}',  # Unix timestamps
        r'\d{2}:\d{2}:\d{2}',  # Time
    ]

    timestamp_count = sum(
        len(re.findall(pattern, '\n'.join(lines[:50])))
        for pattern in timestamp_patterns
    )

    if timestamp_count > 10:
        return "chronological"

    # Topical indicators (sections, headers)
    section_markers = len(re.findall(r'^(#{1,6}\s+|={3,}|-{3,}|[A-Z][^a-z]+$)', content, re.MULTILINE))
    if section_markers > 5:
        return "topical"

    # Check for structured data (CSV, JSON)
    if content.count(',') > len(lines) or content.count('"') > len(lines) * 2:
        return "random"  # Likely tabular/structured, treat as random access

    # Default: assume topical for text with some structure
    if section_markers > 0:
        return "topical"

    return "random"


def _detect_separators(lines: list[str], content: str) -> list[dict]:
    """Detect potential custom separators for chunking."""
    separators = []

    # Check for common separator patterns
    patterns = [
        (r'^---+$', '---', 'Dash line (conversation/section separator)'),
        (r'^===+$', '===', 'Equal line (strong section separator)'),
        (r'^\*\*\*+$', '***', 'Asterisk line (Markdown separator)'),
        (r'^\s*####+', '####', 'Markdown headers (sections)'),
        (r'^\s*##', '##', 'Markdown headers (sections)'),
        (r'^```', '```', 'Code block boundaries'),
    ]

    for pattern, literal, description in patterns:
        matches = len(re.findall(pattern, content, re.MULTILINE))
        if matches > 2:  # Need at least 3 to create 2 chunks
            avg_chunk_size = len(content) // (matches + 1) if matches > 0 else 0
            separators.append({
                'pattern': literal,
                'count': matches,
                'description': description,
                'avg_chunk_size': avg_chunk_size,
                'avg_chunk_tokens': avg_chunk_size // 4,
            })

    # Check for double newlines (paragraph separator)
    double_newline_count = content.count('\n\n')
    if double_newline_count > 5:
        avg_chunk_size = len(content) // (double_newline_count + 1)
        separators.append({
            'pattern': '\\n\\n',
            'count': double_newline_count,
            'description': 'Paragraphs (double newline)',
            'avg_chunk_size': avg_chunk_size,
            'avg_chunk_tokens': avg_chunk_size // 4,
        })

    # Sort by average chunk size (larger chunks first = more semantic units)
    separators.sort(key=lambda x: x['avg_chunk_size'], reverse=True)

    return separators


def format_profile_report(profile: dict) -> str:
    """Format profile as human-readable report with policy recommendations."""
    if "error" in profile:
        return f"Error: {profile['error']}"

    # State features
    scale = profile["scale"]
    structure = profile["structure"]
    density = profile["density"]
    line_length = profile["line_length"]
    clustering = profile["clustering"]
    separators = profile.get("separators", [])

    # Policy recommendation based on state
    policy = _recommend_policy(scale, structure, density, line_length, clustering, separators)

    # Budget estimate
    budget_pct = _estimate_budget(scale, structure, density)

    lines = [
        "STATE PROFILE",
        "=" * 60,
        "",
        f"Scale:       {scale} tokens ({profile['est_tokens']:,})",
        f"Structure:   {structure}",
        f"Density:     {density}",
        f"Line length: {line_length} (avg {profile['avg_line_len']} chars)",
        f"Clustering:  {clustering}",
        "",
        "STATS",
        "-" * 60,
        f"File size:   {profile['file_size_bytes']:,} bytes",
        f"Lines:       {profile['num_lines']:,}",
        f"Blank ratio: {profile['blank_ratio']:.1%}",
    ]

    # Add separator information if detected
    if separators:
        lines.append("")
        lines.append("DETECTED SEPARATORS")
        lines.append("-" * 60)
        for sep in separators[:3]:  # Show top 3
            lines.append(f"{sep['description']}: {sep['count']} occurrences")
            lines.append(f"  → {sep['count'] + 1} chunks, ~{sep['avg_chunk_tokens']:,} tokens each")

    lines.extend([
        "",
        "RECOMMENDED POLICY",
        "-" * 60,
        policy,
        "",
        f"Target budget: {budget_pct}% of file",
        "=" * 60,
    ])

    return "\n".join(lines)


def _recommend_policy(scale: str, structure: str, density: str, line_length: str, clustering: str, separators: list | None = None) -> str:
    """Recommend exploration policy based on state features."""
    policies = []

    # Check for strong separator patterns (conversation, sections)
    has_strong_separator = False
    recommended_separator = None
    chunk_count = 0

    if separators:
        # Look for conversation or section separators with good chunk size
        for sep in separators:
            # Conversation markers (---) or strong section markers
            if sep['pattern'] in ['---', '==='] and 500 < sep['avg_chunk_tokens'] < 50000:
                has_strong_separator = True
                recommended_separator = sep
                chunk_count = sep['count'] + 1
                break

    # If strong separator detected, recommend chunk-based strategy
    if has_strong_separator and recommended_separator:
        policies.append("USE SEPARATOR-BASED CHUNKING:")
        policies.append(f"  --separator-regex '^{recommended_separator['pattern']}$'")
        policies.append(f"  --shape 500:{min(chunk_count, 200)}")
        policies.append("")
        policies.append(f"This will compress {chunk_count} chunks to {min(chunk_count, 200)} lines")
        policies.append("(~500 chars per chunk, ideal for conversations/sections)")
        return "\n".join(policies)

    # Otherwise, use standard line-based policies
    # Structure discovery always first
    policies.append("1. Structure discovery (grep, head/tail)")

    # Based on structure
    if structure == "high":
        policies.append("2. Semantic grep for key markers")
        policies.append("3. Spot check samples")
    elif structure == "low":
        policies.append("2. Wide survey (80:120)")
        policies.append("3. Strategic sampling (start/mid/end)")
    else:  # medium
        policies.append("2. Wide survey + semantic grep")

    # Based on density
    if density == "dense":
        policies.append("4. Deep focus on critical sections (200:60)")
    elif density == "sparse":
        policies.append("   → Use aggressive compression")

    # Based on line length
    if line_length == "long":
        policies.append("   → Consider --wrap for horizontal zoom")

    # Based on scale
    if scale == "> 2M":
        policies.append("   → Statistical sampling only")

    return "\n".join(policies)


def _estimate_budget(scale: str, structure: str, density: str) -> str:
    """Estimate reasonable budget percentage."""
    base = 10.0

    # Adjust for scale
    if scale == "< 20K":
        base = 20.0
    elif scale == "20K-200K":
        base = 10.0
    elif scale == "200K-2M":
        base = 5.0
    else:  # > 2M
        base = 1.0

    # Adjust for structure
    if structure == "high":
        base *= 0.5  # Can be very efficient
    elif structure == "low":
        base *= 1.5  # Need more reading

    # Adjust for density
    if density == "dense":
        base *= 1.5
    elif density == "sparse":
        base *= 0.5

    # Cap at reasonable range
    base = max(0.5, min(base, 40.0))

    return f"{base:.1f}"

